<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    
    <title type="html" xml:lang="en">From the Bottom of the Heap - R</title>
    <link type="application/atom+xml" href="https://www.fromthebottomoftheheap.net/feed-R.xml" rel="self"/>
    <link type="text/plain" href="https://www.fromthebottomoftheheap.net" rel="alternate"/>
    <updated>2022-02-16T05:54:03-06:00</updated>
    <id>https://www.fromthebottomoftheheap.net/</id>
    <author>
        <name>Gavin L. Simpson</name>
        <email>ucfagls@gmail.com</email>
        <uri>https://www.fromthebottomoftheheap.net/about.html</uri>
    </author>
    <rights type="text">Copyright 2010&ndash;2022 Gavin L. Simpson. Available under Creative Commons CC-BY licence</rights>
    
    <entry>
        <title>Using random effects in GAMs with mgcv</title>
        <link href="https://www.fromthebottomoftheheap.net/2021/02/02/random-effects-in-gams/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2021-02-02T10:50:00-06:00</published>
        <updated>2021-02-02T10:50:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2021/02/02/random-effects-in-gams/</id>
        
        <summary type="html">
            &lt;p&gt;
There are lots of choices for fitting generalized linear mixed effects models within R, but if you want to include smooth functions of covariates, the choices are limited. One option is to fit the model using &lt;code&gt;gamm()&lt;/code&gt; from the &lt;strong&gt;mgcv&lt;/strong&gt; 📦 or &lt;code&gt;gamm4()&lt;/code&gt; from the &lt;strong&gt;gamm4&lt;/strong&gt; 📦, which use &lt;code&gt;lme()&lt;/code&gt; (&lt;strong&gt;nlme&lt;/strong&gt; 📦) or one of &lt;code&gt;lmer()&lt;/code&gt; or &lt;code&gt;glmer()&lt;/code&gt; (&lt;strong&gt;lme4&lt;/strong&gt; 📦) under the hood respectively. The problem with doing things that way is that you get PQL fitting for non-Gaussian models (😱) and the range of families for handling non-Gaussian responses is quite limited, especially compared with the extended families now available with &lt;code&gt;gam()&lt;/code&gt;. &lt;strong&gt;brms&lt;/strong&gt; 📦 is a good option if you don’t want to do everything by hand, but the MCMC can be slow. Instead, we could use the equivalence between smooths and random effects and use &lt;code&gt;gam()&lt;/code&gt; or &lt;code&gt;bam()&lt;/code&gt; from &lt;strong&gt;mgcv&lt;/strong&gt;. In this post I’ll show you how to do just that.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Getting data from the Canada Covid-19 Tracker using R</title>
        <link href="https://www.fromthebottomoftheheap.net/2021/01/31/getting-data-from-canada-covid-19-tracker-using-r/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2021-01-31T10:49:00-06:00</published>
        <updated>2021-01-31T10:49:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2021/01/31/getting-data-from-canada-covid-19-tracker-using-r/</id>
        
        <summary type="html">
            &lt;p&gt;
Last semester (Fall 2020) I taught a new course in healthcare data science for the &lt;a href=&quot;https://www.schoolofpublicpolicy.sk.ca/&quot;&gt;Johnson Shoyama Graduate School in Public Policy&lt;/a&gt;. One of the final topics of the course was querying application programming interfaces (APIs) from within R. The example we used was querying data on the Covid 19 pandemic from the &lt;a href=&quot;https://covid19tracker.ca&quot;&gt;Covid-19 Tracker Canada&lt;/a&gt;, which has a simple API that’s easy to work with. In this post I’ll show how we accessed the API from within R and converted the query responses into something we can work with easily.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Two new versions of gratia released</title>
        <link href="https://www.fromthebottomoftheheap.net/2021/01/30/two-new-versions-of-gratia-on-cran/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2021-01-30T15:00:00-06:00</published>
        <updated>2021-01-30T15:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2021/01/30/two-new-versions-of-gratia-on-cran/</id>
        
        <summary type="html">
            &lt;p&gt;
While the Covid-19 pandemic and teaching a new course in the fall put paid to most of my development time last year, some time off work this January allowed me time to work on &lt;strong&gt;gratia&lt;/strong&gt; 📦 again. I released 0.5.0 to CRAN in part to fix an issue with tests not running on the new M1 chips from Apple because I wasn’t using &lt;strong&gt;vdiffr&lt;/strong&gt; 📦 conditionally. Version 0.5.1 followed shortly thereafter as I’d messed up an argument name in &lt;code&gt;smooth_estimates()&lt;/code&gt;, a new function that I hope will allow development to proceed more quickly and mke it easier to maintain code and extend functionality to cover a greater range of model types. Read on to fin out more about &lt;code&gt;smooth_estimates()&lt;/code&gt; and what else was in these two releases.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Extrapolating with B splines and GAMs</title>
        <link href="https://www.fromthebottomoftheheap.net/2020/06/03/extrapolating-with-gams/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2020-06-03T13:00:00-06:00</published>
        <updated>2020-06-03T13:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2020/06/03/extrapolating-with-gams/</id>
        
        <summary type="html">
            &lt;p&gt;
An issue that often crops up when modelling with generlaized additive models (GAMs), especially with time series or spatial data, is how to extrapolate beyond the range of the data used to train the model? The issue arises because GAMs use splines to learn from the data using basis functions. The splines themselves are built from basis functions that are typically setup in terms of the data used to fit the model. If there are no basis functions beyond the range of the input data, what exactly is being used if we want to extrapolate? A related issue is that of the wiggliness penalty; depending on the type of basis used, the penalty could extend over the entire real line (-∞–∞), or only over the range of the input data. In this post I want to take a practical look the extrapolation behaviour of splines in GAMs fitted with the &lt;strong&gt;mgcv&lt;/strong&gt; package for R. In particular I want to illustrate how flexible the B spline basis is.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>gratia 0.4.1 released</title>
        <link href="https://www.fromthebottomoftheheap.net/2020/05/31/new-gratia-release/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2020-05-31T07:00:00-06:00</published>
        <updated>2020-05-31T07:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2020/05/31/new-gratia-release/</id>
        
        <summary type="html">
            &lt;p&gt;
After a slight snafu related to the 1.0.0 release of &lt;strong&gt;dplyr&lt;/strong&gt;, a new version of &lt;strong&gt;gratia&lt;/strong&gt; is out and available on CRAN. This release brings a number of new features, including differences of smooths, partial residuals on partial plots of univariate smooths, and a number of utility functions, while under the hood &lt;strong&gt;gratia&lt;/strong&gt; works for a wider range of models that can be fitted by &lt;strong&gt;mgcv&lt;/strong&gt;.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Rendering your README with GitHub Actions</title>
        <link href="https://www.fromthebottomoftheheap.net/2020/04/30/rendering-your-readme-with-github-actions/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2020-04-30T14:30:00-06:00</published>
        <updated>2020-04-30T14:30:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2020/04/30/rendering-your-readme-with-github-actions/</id>
        
        <summary type="html">
            &lt;p&gt;
There’s one thing that has bugged me for a while about developing R packages. We have all these nice, modern tools we have for tracking our code, producing web sites from the &lt;strong&gt;roxygen&lt;/strong&gt; documentation, an so on. Yet for every code commit I make to the master branch of a package repo, there’s often two or more additional steps I need to take to keep the package &lt;code&gt;README.md&lt;/code&gt; and &lt;em&gt;pkgdown&lt;/em&gt; site in sync with the code. Don’t get me wrong; it’s amazing that we have these tools available to help users get to grips with our R packages. It’s just that there’s a lot of extra things to remember to do to keep everything up to date. The development of free-to-use services such as Travis CI or Appveyor have been very useful as they can automate many of these repetitive tasks. A more recent newcomer to the field is &lt;a href=&quot;https://github.com/features/actions&quot;&gt;GitHub Actions&lt;/a&gt;. The other day I was grappling with getting a GitHub Actions workflow to render a &lt;code&gt;README.Rmd&lt;/code&gt; file to &lt;code&gt;README.md&lt;/code&gt; on GitHub, so that I didn’t have to do it locally all the time. After a lot of trial and error, this is how I got it working.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Pivoting tidily</title>
        <link href="https://www.fromthebottomoftheheap.net/2019/10/25/pivoting-tidily/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2019-10-25T00:00:00-06:00</published>
        <updated>2019-10-25T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2019/10/25/pivoting-tidily/</id>
        
        <summary type="html">
            &lt;p&gt;
One of the fun bits of my job is that I have actual time dedicated to helping colleagues and grad students with statistical or computational problems. Recently I’ve been helping one of our Lab Instructors with some data that from their Plant Physiology Lab course. Whilst I was writing some R code to import the raw data for the lab from an Excel sheet, it occurred to me that this would be a good excuse to look at the new &lt;code&gt;pivot_longer()&lt;/code&gt; and &lt;code&gt;pivot_wider()&lt;/code&gt; functions from the &lt;em&gt;tidyr&lt;/em&gt; package. In this post I show how these new functions facilitate common data processing steps; I was personally surprised how little data wrangling was actually needed in the end to read in the data from the lab.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>radian: a modern console for R</title>
        <link href="https://www.fromthebottomoftheheap.net/2019/06/18/radian-console-for-r/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2019-06-18T00:00:00-06:00</published>
        <updated>2019-06-18T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2019/06/18/radian-console-for-r/</id>
        
        <summary type="html">
            &lt;p&gt;
Whenever I’m developing R code or writing data wrangling or analysis scripts for research projects that I work on I use &lt;em&gt;Emacs&lt;/em&gt; and its add-on package &lt;a href=&quot;https://ess.r-project.org/&quot;&gt;&lt;em&gt;Emacs Speaks Statistics&lt;/em&gt;&lt;/a&gt; (&lt;em&gt;ESS&lt;/em&gt;). I’ve done so for nigh on a couple of decades now, ever since I switched full time to running Linux as my daily OS. For years this has served me well, though I wouldn’t call myself an &lt;em&gt;Emacs&lt;/em&gt; expert; not even close! With a bit of help from some R Core coding standards document I got indentation working how I like it, I learned to contort my fingers in weird and wonderful ways to execute a small set of useful shortcuts, and I even committed some of those shortcuts to memory. More recently, however, my go-to methods for configuring &lt;em&gt;Emacs+ESS&lt;/em&gt; were failing; indentation was all over the shop, the smart &lt;code&gt;_&lt;/code&gt; stopped working or didn’t work as it had for over a decade, syntax highlighting of R-related files, like &lt;code&gt;.Rmd&lt;/code&gt; was hit and miss, and &lt;em&gt;polymode&lt;/em&gt; was just a mystery to me. Configuring &lt;em&gt;Emacs+ESS&lt;/em&gt; was becoming much more of a chore, and rather unhelpfully, my problems coincided with my having less and less time to devote to tinkering with my computer setups. Also, fiddling with this stuff just wasn’t fun any more. So, in a fit of pique following one to many reconfiguration sessions of &lt;em&gt;Emacs+ESS&lt;/em&gt;, I went in search of some greener grass. During that search I came across &lt;a href=&quot;https://github.com/randy3k/radian&quot;&gt;radian&lt;/a&gt;, a neat, attractive, simple console for working with R.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Tibbles, checking examples, &amp; character encodings</title>
        <link href="https://www.fromthebottomoftheheap.net/2019/01/22/using-tibbles-and-example-checking/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2019-01-22T07:00:00-06:00</published>
        <updated>2019-01-22T07:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2019/01/22/using-tibbles-and-example-checking/</id>
        
        <summary type="html">
            &lt;p&gt;
Recently I’ve been preparing my &lt;a href=&quot;https://gavinsimpson.github.io/gratia/&quot;&gt;&lt;strong&gt;gratia&lt;/strong&gt; package&lt;/a&gt; for submission to CRAN. During my pre-flight testing I noticed an issue under Windows checking the examples in the package against the reference output I generated on linux. In the latest release of the &lt;a href=&quot;https://tibble.tidyverse.org/&quot;&gt;&lt;strong&gt;tibble&lt;/strong&gt; package&lt;/a&gt;, the way tibbles are printed has changed subtly and in a way that leads to cross-platform differences. As I write this, tibbles with more than a set number of rows are printed in a truncated form, showing only the first 10 rows of data. In such cases, a final line is printed with an ellipsis and a note as to how many more rows are in the tibble. It was this ellipsis that was causing the cross-platform issue where differences between the output generated on windows and the reference output were being identified during &lt;code&gt;R CMD check&lt;/code&gt; on Windows. If this is causing you an issue, here’s one way to solve the problem.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Confidence intervals for GLMs</title>
        <link href="https://www.fromthebottomoftheheap.net/2018/12/10/confidence-intervals-for-glms/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2018-12-10T08:00:00-06:00</published>
        <updated>2018-12-10T08:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2018/12/10/confidence-intervals-for-glms/</id>
        
        <summary type="html">
            &lt;p&gt;
You’ve estimated a GLM or a related model (GLMM, GAM, etc.) for your latest paper and, like a good researcher, you want to visualise the model and show the uncertainty in it. In general this is done using confidence intervals with typically 95% converage. If you remember a little bit of theory from your stats classes, you may recall that such an interval can be produced by adding to and subtracting from the fitted values 2 times their standard error. Unfortunately this only really works like this for a linear model. If I had a dollar (even a Canadian one) for every time I’ve seen someone present graphs of estimated abundance of some species where the confidence interval includes negative abundances, I’d be rich! Here, following the rule of “if I’m asked more than once I should write a blog post about it!” I’m going to show a simple way to correctly compute a confidence interval for a GLM or a related model.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Introducing gratia</title>
        <link href="https://www.fromthebottomoftheheap.net/2018/10/23/introducing-gratia/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2018-10-23T06:00:00-06:00</published>
        <updated>2018-10-23T06:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2018/10/23/introducing-gratia/</id>
        
        <summary type="html">
            &lt;p&gt;
I use generalized additive models (GAMs) in my research work. I use them a lot! Simon Wood’s &lt;strong&gt;mgcv&lt;/strong&gt; package is an excellent set of software for specifying, fitting, and visualizing GAMs for very large data sets. Despite recently dabbling with &lt;strong&gt;brms&lt;/strong&gt;, &lt;strong&gt;mgcv&lt;/strong&gt; is still my go-to GAM package. The only down-side to &lt;strong&gt;mgcv&lt;/strong&gt; is that it is not very tidy-aware and the &lt;strong&gt;ggplot&lt;/strong&gt;-verse may as well not exist as far as it is concerned. This in itself is no bad thing, though as someone who uses &lt;strong&gt;mgcv&lt;/strong&gt; a lot but also prefers to do my plotting with &lt;strong&gt;ggplot2&lt;/strong&gt;, this lack of awareness was starting to hurt. So, I started working on something to help bridge the gap between these two separate worlds that I inhabit. The fruit of that labour is &lt;strong&gt;gratia&lt;/strong&gt;, and development has progressed to the stage where I am ready to talk a bit more about it.
&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;gratia&lt;/strong&gt; is an R package for working with GAMs fitted with &lt;code&gt;gam()&lt;/code&gt;, &lt;code&gt;bam()&lt;/code&gt; or &lt;code&gt;gamm()&lt;/code&gt; from &lt;strong&gt;mgcv&lt;/strong&gt; or &lt;code&gt;gamm4()&lt;/code&gt; from the &lt;strong&gt;gamm4&lt;/strong&gt; package, although functionality for handling the latter is not yet implement. &lt;strong&gt;gratia&lt;/strong&gt; provides functions to replace the base-graphics-based &lt;code&gt;plot.gam()&lt;/code&gt; and &lt;code&gt;gam.check()&lt;/code&gt; that &lt;strong&gt;mgcv&lt;/strong&gt; provides with &lt;strong&gt;ggplot2&lt;/strong&gt;-based versions. Recent changes have also resulted in &lt;strong&gt;gratia&lt;/strong&gt; being much more &lt;strong&gt;tidyverse&lt;/strong&gt; aware and it now (mostly) returns outputs as tibbles.
&lt;/p&gt;
&lt;p&gt;
In this post I wanted to give a flavour of what is currently possible with &lt;strong&gt;gratia&lt;/strong&gt; and outline what still needs to be implemented.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Fitting GAMs with brms: part 1</title>
        <link href="https://www.fromthebottomoftheheap.net/2018/04/21/fitting-gams-with-brms/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2018-04-21T04:00:00-06:00</published>
        <updated>2018-04-21T04:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2018/04/21/fitting-gams-with-brms/</id>
        
        <summary type="html">
            &lt;p&gt;
Regular readers will know that I have a somewhat unhealthy relationship with GAMs and the &lt;strong&gt;mgcv&lt;/strong&gt; package. I use these models all the time in my research but recently we’ve been hitting the limits of the range of models that &lt;strong&gt;mgcv&lt;/strong&gt; can fit. So I’ve been looking into alternative ways to fit the GAMs I want to fit but which can handle the kinds of data or distributions that have been cropping up in our work. The &lt;strong&gt;brms&lt;/strong&gt; package &lt;span class=&quot;citation&quot; data-cites=&quot;brms-2017&quot;&gt;(Bürkner, 2017)&lt;/span&gt; is an excellent resource for modellers, providing a high-level R front end to a vast array of model types, all fitted using &lt;a href=&quot;http://mc-stan.org&quot;&gt;Stan&lt;/a&gt;. &lt;strong&gt;brms&lt;/strong&gt; is the perfect package to go beyond the limits of &lt;strong&gt;mgcv&lt;/strong&gt; because &lt;strong&gt;brms&lt;/strong&gt; even uses the smooth functions provided by &lt;strong&gt;mgcv&lt;/strong&gt;, making the transition easier. In this post I take a look at how to fit a simple GAM in &lt;strong&gt;brms&lt;/strong&gt; and compare it with the same model fitted using &lt;strong&gt;mgcv&lt;/strong&gt;.
&lt;/p&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-brms-2017&quot;&gt;
&lt;p&gt;
Bürkner, P.-C. (2017). brms: An R package for bayesian multilevel models using Stan. &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 80, 1–28. doi:&lt;a href=&quot;https://doi.org/10.18637/jss.v080.i01&quot;&gt;10.18637/jss.v080.i01&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Comparing smooths in factor-smooth interactions II</title>
        <link href="https://www.fromthebottomoftheheap.net/2017/12/14/difference-splines-ii/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2017-12-14T10:00:00-06:00</published>
        <updated>2017-12-14T10:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2017/12/14/difference-splines-ii/</id>
        
        <summary type="html">
            &lt;p&gt;
In a &lt;a href=&quot;https://www.fromthebottomoftheheap.net/2017/10/10/difference-splines-i/&quot;&gt;previous post&lt;/a&gt; I looked at an approach for computing the differences between smooths estimated as part of a factor-smooth interaction using &lt;code&gt;s()&lt;/code&gt;’s &lt;code&gt;by&lt;/code&gt; argument. When a common-or-garden factor variable is passed to &lt;code&gt;by&lt;/code&gt;, &lt;code&gt;gam()&lt;/code&gt; estimates a separate smooth for each &lt;em&gt;level&lt;/em&gt; of the &lt;code&gt;by&lt;/code&gt; factor. Using the &lt;span class=&quot;math inline&quot;&gt;(Xp)&lt;/span&gt; matrix approach, we previously saw that we can post-process the model to generate estimates for pairwise differences of smooths. However, the &lt;code&gt;by&lt;/code&gt; variable approach of estimating a separate smooth for each level of the factor my be quite inefficient in terms of degrees of freedom used by the model. This is especially so in situations where the estimated curves are quite similar but wiggly; why estimate many separate wiggly smooths when one, plus some simple difference smooths, will do the job just as well? In this post I look at an alternative to estimating separate smooths using an &lt;em&gt;ordered&lt;/em&gt; factor for the &lt;code&gt;by&lt;/code&gt; variable.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>First steps with MRF smooths</title>
        <link href="https://www.fromthebottomoftheheap.net/2017/10/19/first-steps-with-mrf-smooths/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2017-10-19T12:00:00-06:00</published>
        <updated>2017-10-19T12:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2017/10/19/first-steps-with-mrf-smooths/</id>
        
        <summary type="html">
            &lt;p&gt;
One of the specialist smoother types in the &lt;strong&gt;mgcv&lt;/strong&gt; package is the Markov Random Field (MRF) smooth. This smoother essentially allows you to model spatial data with an intrinsic Gaussian Markov random field (GMRF). GRMFs are often used for spatial data measured over discrete spatial regions. MRFs are quite flexible as you can think about them as representing an undirected graph whose nodes are your samples and the connections between the nodes are specified via a neighbourhood structure. I’ve become interested in using these MRF smooths to include information about relationships between species. However, these smooths are not widely documented in the smoothing literature so working out how best to use them to do what we want has been a little tricky once you move beyond the typical spatial examples. As a result I’ve been fiddling with these smooths, fitting them to some spatial data I came across in a tutorial &lt;a href=&quot;https://pudding.cool/process/regional_smoothing/&quot;&gt;Regional Smoothing in R&lt;/a&gt; from The Pudding. In this post I take a quick look at how to use the MRF smooth in &lt;strong&gt;mgcv&lt;/strong&gt; to model a discrete spatial data set from the US Census Bureau.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Comparing smooths in factor-smooth interactions I</title>
        <link href="https://www.fromthebottomoftheheap.net/2017/10/10/difference-splines-i/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2017-10-10T16:00:00-06:00</published>
        <updated>2017-10-10T16:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2017/10/10/difference-splines-i/</id>
        
        <summary type="html">
            &lt;p&gt;
One of the really appealing features of the &lt;strong&gt;mgcv&lt;/strong&gt; package for fitting GAMs is the functionality it exposes for fitting quite complex models, models that lie well beyond what many of us may have learned about what GAMs can do. One of those features that I use a lot is the ability to model the smooth effects of some covariate &lt;span class=&quot;math inline&quot;&gt;(x)&lt;/span&gt; in the different levels of a factor. Having estimated a separate smoother for each level of the factor, the obvious question is, which smooths are different? In this post I’ll take a look at one way to do this using &lt;code&gt;by&lt;/code&gt;-variable smooths.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Fitting count and zero-inflated count GLMMs with mgcv</title>
        <link href="https://www.fromthebottomoftheheap.net/2017/05/04/compare-mgcv-with-glmmTMB/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2017-05-04T13:45:00-06:00</published>
        <updated>2017-05-04T13:45:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2017/05/04/compare-mgcv-with-glmmTMB/</id>
        
        <summary type="html">
            &lt;p&gt;
A couple of days ago, Mollie Brooks and coauthors posted a &lt;a href=&quot;http://doi.org/10.1101/132753&quot;&gt;preprint&lt;/a&gt; on &lt;a href=&quot;http://biorxiv.org/&quot;&gt;BioRχiv&lt;/a&gt; illustrating the use of the &lt;strong&gt;glmmTMB&lt;/strong&gt; R package for fitting zero-inflated GLMMs &lt;span class=&quot;citation&quot; data-cites=&quot;Brooks2017-so&quot;&gt;(Brooks et al., 2017)&lt;/span&gt;. In the paper, &lt;strong&gt;glmmTMB&lt;/strong&gt; is compared with several other GLMM-fitting packages. &lt;strong&gt;mgcv&lt;/strong&gt; has recently gained the ability to fit a wider range of families beyond the exponential family of distributions, including zero-inflated Poisson models. &lt;strong&gt;mgcv&lt;/strong&gt; can also fit simple GLMMs through a spline equivalent of a Gaussian random effect. So, whilst I was waiting on some Bayesian GAMs to finish sampling, I decided to see how &lt;strong&gt;mgcv&lt;/strong&gt; compared against &lt;strong&gt;glmmTMB&lt;/strong&gt; on the two examples used in the paper.
&lt;/p&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-Brooks2017-so&quot;&gt;
&lt;p&gt;
Brooks, M. E., Kristensen, K., Benthem, K. J. van, Magnusson, A., Berg, C. W., Nielsen, A., et al. (2017). Modeling Zero-Inflated count data with glmmTMB. &lt;em&gt;bioRxiv&lt;/em&gt;, 132753. doi:&lt;a href=&quot;https://doi.org/10.1101/132753&quot;&gt;10.1101/132753&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Prediction intervals for GLMs part II</title>
        <link href="https://www.fromthebottomoftheheap.net/2017/05/01/glm-prediction-intervals-ii/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2017-05-01T09:00:00-06:00</published>
        <updated>2017-05-01T09:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2017/05/01/glm-prediction-intervals-ii/</id>
        
        <summary type="html">
            &lt;p&gt;
One of my more popular &lt;a href=&quot;http://stackoverflow.com/a/14424417/429846&quot;&gt;answers&lt;/a&gt; on StackOverflow concerns the issue of prediction intervals for a generalized linear model (GLM). Comments, even on StackOverflow, aren’t a good place for a discussion so I thought I’d post something hereon my blog that went into a bit more detail as to why, for some common types of GLMs, prediction intervals aren’t that useful and require a lot more thinking about what they mean and how they should be calculated. I’ve broken it into two and in this, the second part, I look at Possion models.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Prediction intervals for GLMs part I</title>
        <link href="https://www.fromthebottomoftheheap.net/2017/05/01/glm-prediction-intervals-i/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2017-05-01T08:45:00-06:00</published>
        <updated>2017-05-01T08:45:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2017/05/01/glm-prediction-intervals-i/</id>
        
        <summary type="html">
            &lt;p&gt;
One of my more popular &lt;a href=&quot;http://stackoverflow.com/a/14424417/429846&quot;&gt;answers&lt;/a&gt; on StackOverflow concerns the issue of prediction intervals for a generalized linear model (GLM). My answer really only addresses how to compute confidence intervals for parameters but in the comments I discuss the more substantive points raised by the OP in their question. Lately there’s been a bit of back and forth between Jarrett Byrnes and myself about what a prediction “interval” for a GLM might mean. Comments, even on StackOverflow, aren’t a good place for a discussion so I thought I’d post something here that went into a bit more detail as to why, for some common types of GLMs, prediction intervals aren’t that useful and require a lot more thinking about what they mean and how they should be calculated. For illustration, I thought I’d use some small teaching example data sets, but whilst writing the post it started to get a little on the long side. So, I’ve broken it into two and in this part I look at logistic regression.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Simultaneous intervals for derivatives of smooths revisited</title>
        <link href="https://www.fromthebottomoftheheap.net/2017/03/21/simultaneous-intervals-for-derivatives-of-smooths/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2017-03-21T00:00:00-06:00</published>
        <updated>2017-03-21T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2017/03/21/simultaneous-intervals-for-derivatives-of-smooths/</id>
        
        <summary type="html">
            &lt;p&gt;
Eighteen months ago &lt;a href=&quot;https://www.fromthebottomoftheheap.net/2016/12/15/simultaneous-interval-revisited/&quot;&gt;I screwed up&lt;/a&gt;! I’d written a &lt;a href=&quot;https://www.fromthebottomoftheheap.net/2014/06/16/simultaneous-confidence-intervals-for-derivatives/&quot;&gt;post&lt;/a&gt; in which I described the use of simulation from the posterior distribution of a fitted GAM to derive simultaneous confidence intervals for the derivatives of a penalized spline. It was a nice post that attracted some interest. It was also wrong. In December I corrected the first part of that mistake by illustrating one approach to compute an actual simultaneous interval, but only for the fitted smoother. At the time I thought that the approach I outlined would translate to the derivatives but I was being lazy then Christmas came and went and I was back to teaching — you know how it goes. Anyway, in this post I hope to finally rectify my past stupidity and show how the approach used to generate simultaneous intervals from the December 2016 post can be applied to the derivatives of a spline.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Modelling extremes using generalized additive models</title>
        <link href="https://www.fromthebottomoftheheap.net/2017/01/25/modelling-extremes-with-gams/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2017-01-25T00:00:00-06:00</published>
        <updated>2017-01-25T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2017/01/25/modelling-extremes-with-gams/</id>
        
        <summary type="html">
            &lt;p&gt;
Quite some years ago, whilst working on the EU Sixth Framework project &lt;em&gt;Euro-limpacs&lt;/em&gt;, I organized a workshop on statistical methods for analyzing time series data. One of the sessions was on the analysis of extremes, ably given by Paul Northrop (UCL Department of Statistical Science). That intro certainly whet my appetite but I never quite found the time to dig into the arcane world of extreme value theory. Two recent events rekindled my interest in extremes; Simon Wood quietly introduced into his &lt;strong&gt;mgcv&lt;/strong&gt; package a family function for the generalized extreme value distribution (GEV), and I was asked to review a paper on extremes in time series. Since then I’ve been investigating options for fitting models for extremes to environmental time series, especially those that allow for time-varying effects of covariates on the parameters of the GEV. One of the first things I did was sit down with &lt;strong&gt;mgcv&lt;/strong&gt; to get a feel for the &lt;code&gt;gevlss()&lt;/code&gt; family function that Simon had added to the package by repeating an analysis of a classic example data set that had been performed using the &lt;strong&gt;VGAM&lt;/strong&gt; package of Thomas Yee.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Pangaea and R and open palaeo data</title>
        <link href="https://www.fromthebottomoftheheap.net/2016/12/16/pangaea-r-open-palaeo-data/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2016-12-16T00:00:00-06:00</published>
        <updated>2016-12-16T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2016/12/16/pangaea-r-open-palaeo-data/</id>
        
        <summary type="html">
            &lt;p&gt;
For a while now, I’ve been wanting to experiment with rOpenSci’s &lt;strong&gt;pangaear&lt;/strong&gt; package &lt;span class=&quot;citation&quot; data-cites=&quot;pangaear-024&quot;&gt;(Chamberlain et al., 2016)&lt;/span&gt;, which allows you to search, and download data from, the Pangaea, a major data repository for the earth and environmental sciences. Earlier in the year, as a member of the editorial board of &lt;a href=&quot;http://www.nature.com/sdata/&quot;&gt;Scientific Data&lt;/a&gt;, Springer Nature’s open data journal I was handling a data descriptor submission that described a new 2,200-year foraminiferal δ&lt;sup&gt;&lt;sup&gt;18&lt;/sup&gt;&lt;/sup&gt;O record from the Gulf of Taranto in the Ionian Sea &lt;span class=&quot;citation&quot; data-cites=&quot;Taricco2016-pv&quot;&gt;(Taricco et al., 2016)&lt;/span&gt;. The data descriptor was recently &lt;a href=&quot;http://doi.org/10.1038/sdata.2016.42&quot;&gt;published&lt;/a&gt; and as part of the submission Carla Taricco deposited the data set in Pangaea. So, what better opportunity to test out &lt;strong&gt;pangaear&lt;/strong&gt;? (Oh and to fit a GAM to the data while I’m at it!)
&lt;/p&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-pangaear-024&quot;&gt;
&lt;p&gt;
Chamberlain, S., Woo, K., MacDonald, A., Zimmerman, N., and Simpson, G. (2016). &lt;em&gt;Pangaear: Client for the ’pangaea’ database&lt;/em&gt;. Available at: &lt;a href=&quot;https://CRAN.R-project.org/package=pangaear&quot;&gt;https://CRAN.R-project.org/package=pangaear&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Taricco2016-pv&quot;&gt;
&lt;p&gt;
Taricco, C., Alessio, S., Rubinetti, S., Vivaldo, G., and Mancuso, S. (2016). A foraminiferal &lt;span class=&quot;math inline&quot;&gt;()&lt;/span&gt;18O record covering the last 2,200 years. &lt;em&gt;Scientific Data&lt;/em&gt; 3, 160042. doi:&lt;a href=&quot;https://doi.org/10.1038/sdata.2016.42&quot;&gt;10.1038/sdata.2016.42&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Simultaneous intervals for smooths revisited</title>
        <link href="https://www.fromthebottomoftheheap.net/2016/12/15/simultaneous-interval-revisited/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2016-12-15T00:00:00-06:00</published>
        <updated>2016-12-15T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2016/12/15/simultaneous-interval-revisited/</id>
        
        <summary type="html">
            &lt;p&gt;
Eighteen months ago I &lt;a href=&quot;https://www.fromthebottomoftheheap.net/2014/06/16/simultaneous-confidence-intervals-for-derivatives/&quot;&gt;wrote a post&lt;/a&gt; in which I described the use of simulation from the posterior distribution of a fitted GAM to derive simultaneous confidence intervals for the derivatives of a penalised spline. It was a nice post that attracted some interest. It was also wrong. I have no idea what I was thinking when I thought the intervals described in that post were simultaneous. Here I hope to rectify that past mistake.
&lt;/p&gt;
&lt;p&gt;
I’ll tackle the issue of simultaneous intervals for the derivatives of penalised spline in a follow-up post. Here, I demonstrate one way to compute a simultaneous interval for a penalised spline in a fitted GAM. As example data, I’ll use the strontium isotope data set included in the &lt;strong&gt;SemiPar&lt;/strong&gt; package, and which is extensively analyzed in the monograph &lt;em&gt;Semiparametric Regression&lt;/em&gt; &lt;span class=&quot;citation&quot; data-cites=&quot;Ruppert2003-pt&quot;&gt;(Ruppert et al., 2003)&lt;/span&gt;. First, load the packages we’ll need as well as the data, which is data set &lt;code&gt;fossil&lt;/code&gt;. If you don’t have &lt;strong&gt;SemiPar&lt;/strong&gt; installed, install it using &lt;code&gt;install.packages(“SemiPar”)&lt;/code&gt; before proceeding
&lt;/p&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-Ruppert2003-pt&quot;&gt;
&lt;p&gt;
Ruppert, D., Wand, M. P., and Carroll, R. J. (2003). &lt;em&gt;Semiparametric regression&lt;/em&gt;. Cambridge University Press.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Rootograms</title>
        <link href="https://www.fromthebottomoftheheap.net/2016/06/07/rootograms/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2016-06-07T00:00:00-06:00</published>
        <updated>2016-06-07T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2016/06/07/rootograms/</id>
        
        <summary type="html">
            &lt;p&gt;
Assessing the fit of a count regression model is not necessarily a straightforward enterprise; often we just look at residuals, which invariably contain patterns of some form due to the discrete nature of the observations, or we plot observed versus fitted values as a scatter plot. Recently, while perusing the latest statistics offerings on ArXiv I came across &lt;span class=&quot;citation&quot; data-cites=&quot;Kleiber2016-pt&quot;&gt;Kleiber and Zeileis (2016)&lt;/span&gt; who propose the &lt;em&gt;rootogram&lt;/em&gt; as an improved approach to the assessment of fit of a count regression model. &lt;a href=&quot;http://arxiv.org/abs/1605.01311&quot;&gt;The paper&lt;/a&gt; is illustrated using R and the authors’ &lt;strong&gt;countreg&lt;/strong&gt; package (currently on R-Forge only). Here, I thought I’d take a quick look at the rootogram with some simulated species abundance data.
&lt;/p&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-Kleiber2016-pt&quot;&gt;
&lt;p&gt;
Kleiber, C., and Zeileis, A. (2016). Visualizing count data regressions using rootograms. Available at: &lt;a href=&quot;http://arxiv.org/abs/1605.01311&quot;&gt;http://arxiv.org/abs/1605.01311&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Harvesting more Canadian climate data</title>
        <link href="https://www.fromthebottomoftheheap.net/2016/05/24/harvesting-more-canadian-climate-data/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2016-05-24T00:00:00-06:00</published>
        <updated>2016-05-24T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2016/05/24/harvesting-more-canadian-climate-data/</id>
        
        <summary type="html">
            &lt;p&gt;
A &lt;a href=&quot;/2015/01/14/harvesting-canadian-climate-data/&quot;&gt;while back I wrote&lt;/a&gt; some code to download climate data from Government of Canada’s historical climate/weather data website for one of our students. In May this year (2016) the Government of Canada changed their website a little and the API code that responded to requests had changed URL and some of the GET parameters had also changed. In fixing those functions I also noted that the original code only downloaded hourly data and not all useful weather variables are recorded hourly; precipitation for example is only in the daily and monthly data formats. This post updates the earlier one, explaining what changed and how the code has been updated. As an added benefit, the functions can now handle downloading daily and monthly data files as well as the hourly files that the original could handle.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>A new default plot for multivariate dispersions</title>
        <link href="https://www.fromthebottomoftheheap.net/2016/04/17/new-plot-default-for-betadisper/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2016-04-17T00:00:00-06:00</published>
        <updated>2016-04-17T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2016/04/17/new-plot-default-for-betadisper/</id>
        
        <summary type="html">
            &lt;p&gt;
This weekend, prompted by a pull request from Michael Friendly, I finally got round to improving the &lt;code&gt;plot&lt;/code&gt; method for &lt;code&gt;betadisper()&lt;/code&gt; in the &lt;strong&gt;vegan&lt;/strong&gt; package. &lt;code&gt;betadisper()&lt;/code&gt; is an implementation of Marti Anderson’s &lt;span class=&quot;smallcaps&quot;&gt;Permdisp&lt;/span&gt; method, a multivariate analogue of Levene’s test for homogeneity of variances. In improving the default plot and allowing customisation of plot features, I was reminded of how much I dislike programming plot functions that use base graphics. But don’t worry, this isn’t going to degenerate into a &lt;strong&gt;ggplot&lt;/strong&gt; love-in nor a &lt;a href=&quot;http://varianceexplained.org/r/why-I-use-ggplot2/&quot;&gt;David Robinson-esque dig&lt;/a&gt; at &lt;a href=&quot;http://simplystatistics.org/2016/02/11/why-i-dont-use-ggplot2/&quot;&gt;Jeff Leek&lt;/a&gt;.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Soap-film smoothers &amp; lake bathymetries</title>
        <link href="https://www.fromthebottomoftheheap.net/2016/03/27/soap-film-smoothers/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2016-03-27T00:00:00-06:00</published>
        <updated>2016-03-27T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2016/03/27/soap-film-smoothers/</id>
        
        <summary type="html">
            &lt;p&gt;
A number of years ago, whilst I was still working at &lt;a href=&quot;http://www.ensis.org.uk/&quot;&gt;ENSIS&lt;/a&gt;, the consultancy arm of the &lt;a href=&quot;http://www.ensis.org.uk/&quot;&gt;ECRC&lt;/a&gt; at &lt;a href=&quot;http://www.ucl.ac.uk&quot;&gt;UCL&lt;/a&gt;, I worked on a project for the (then) Countryside Council for Wales (CCW; now part of &lt;a href=&quot;http://naturalresources.wales&quot;&gt;Natural Resources Wales&lt;/a&gt;). I don’t recall why they were doing this project, but we were tasked with producing a standardised set of bathymetric maps for Welsh lakes. The brief called for the bathymetries to be provided in standard GIS formats. Either CCW’s project manager or the project lead at ENSIS had proposed to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverse_distance_weighting&quot;&gt;inverse distance weighting&lt;/a&gt; (IWD) to smooth the point bathymetric measurements. This probably stemmed from the person that initiatied our bathymetric programme at ENSIS being a GIS wizard, schooled in the ways of ArcGIS. My involvement was mainly data processing of the IDW results. I was however, at the time, also somewhat familiar with the problem of &lt;em&gt;finite area smoothing&lt;/em&gt;&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and had read a paper of Simon Wood’s on his then new soap-film smoother &lt;span class=&quot;citation&quot; data-cites=&quot;Wood2008-gy&quot;&gt;(Wood et al., 2008)&lt;/span&gt;. So, as well as writing scripts to process and present the IDW-based bathymetry data in the report, I snuck a task into the work programme that allowed me to investigate using soap-film smoothers for modelling lake bathymetric data. The timing was never great to write up this method (two children and a move to Canada have occurred since the end of this project), so I’ve not done anything with the idea. Until now…
&lt;/p&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-Wood2008-gy&quot;&gt;
&lt;p&gt;
Wood, S. N., Bravington, M. V., and Hedley, S. L. (2008). Soap film smoothing. &lt;em&gt;Journal of the Royal Statistical Society. Series B, Statistical methodology&lt;/em&gt; 70, 931–955. doi:&lt;a href=&quot;https://doi.org/10.1111/j.1467-9868.2008.00665.x&quot;&gt;10.1111/j.1467-9868.2008.00665.x&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;
&lt;p&gt;
smoothing over a domain with known boundaries, like a lake&lt;a href=&quot;#fnref1&quot; class=&quot;footnote-back&quot;&gt;↩&lt;/a&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Additive modelling global temperature time series: revisited</title>
        <link href="https://www.fromthebottomoftheheap.net/2016/03/25/additive-modeling-global-temperature-series-revisited/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2016-03-25T00:00:00-06:00</published>
        <updated>2016-03-25T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2016/03/25/additive-modeling-global-temperature-series-revisited/</id>
        
        <summary type="html">
            &lt;p&gt;
Quite some time ago, back in 2011, I wrote a &lt;a href=&quot;/2011/06/12/additive-modelling-and-the-hadcrut3v-global-mean-temperature-series/&quot;&gt;post&lt;/a&gt; that used an additive model to fit a smooth trend to the then-current Hadley Centre/CRU global temperature time series data set. Since then the media and scientific papers have been full of reports of record warm temperatures in the past couple of years, of controversies (imagined) regarding data-changes to suit the hypothesis of human induce global warming, and the brouhaha over whether global warming had stalled; the great &lt;a href=&quot;https://en.wikipedia.org/wiki/Global_warming_hiatus&quot;&gt;global warming hiatus or pause&lt;/a&gt;. So it seemed like a good time to revisit that analysis and update it using the latest HadCRUT data.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Are some seasons warming more than others?</title>
        <link href="https://www.fromthebottomoftheheap.net/2015/11/23/are-some-seasons-warming-more-than-others/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2015-11-23T00:00:00-06:00</published>
        <updated>2015-11-23T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2015/11/23/are-some-seasons-warming-more-than-others/</id>
        
        <summary type="html">
            &lt;p&gt;
I ended the &lt;a href=&quot;/2015/11/21/climate-change-and-spline-interactions/&quot;&gt;last post&lt;/a&gt; with some pretty plots of air temperature change within and between years in the &lt;a href=&quot;http://www.metoffice.gov.uk/hadobs/hadcet/&quot;&gt;Central England Temperature series&lt;/a&gt;. The elephant in the room&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; at the end of that post was &lt;em&gt;is the change in the within year (seasonal) effect over time statistically significant?&lt;/em&gt; This is the question I’ll try to answer, or at least show how to answer, now.
&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;
&lt;p&gt;
well, one of the elephants; I also wasn’t happy with the AR(7) for the residuals&lt;a href=&quot;#fnref1&quot; class=&quot;footnote-back&quot;&gt;↩&lt;/a&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Climate change and spline interactions</title>
        <link href="https://www.fromthebottomoftheheap.net/2015/11/21/climate-change-and-spline-interactions/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2015-11-21T00:00:00-06:00</published>
        <updated>2015-11-21T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2015/11/21/climate-change-and-spline-interactions/</id>
        
        <summary type="html">
            &lt;p&gt;
In a series of irregular posts&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; I’ve looked at how additive models can be used to fit non-linear models to time series. Up to now I’ve looked at models that included a single non-linear trend, as well as a model that included a within-year (or seasonal) part and a trend part. In this trend &lt;em&gt;plus&lt;/em&gt; season model it is important to note that the two terms are purely additive; no matter which January you are predicting for in a long timeseries, the seasonal effect for that month will always be the same. The trend part might shift this seasonal contribution up or down a bit, but all January’s are the same. In this post I want to introduce a different type of spline interaction model that will allow us to relax this additivity assumption and fit a model that allows the seasonal part of the model to change in time along with the trend.
&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;
&lt;p&gt;
&lt;a href=&quot;/2011/06/12/additive-modelling-and-the-hadcrut3v-global-mean-temperature-series/&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;/2011/07/21/smoothing-temporally-correlated-data/&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;/2014/05/09/modelling-seasonal-data-with-gam/&quot;&gt;here&lt;/a&gt;&lt;a href=&quot;#fnref1&quot; class=&quot;footnote-back&quot;&gt;↩&lt;/a&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>User-friendly scaling</title>
        <link href="https://www.fromthebottomoftheheap.net/2015/10/08/user-friendly-scaling/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2015-10-08T00:00:00-06:00</published>
        <updated>2015-10-08T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2015/10/08/user-friendly-scaling/</id>
        
        <summary type="html">
            &lt;p&gt;
Back in the mists of time, whilst programming early versions of Canoco, Cajo ter Braak decided to allow users to specify how species and site ordination scores were scaled relative to one another via a simple numeric coding system. This was fine for the DOS-based software that Canoco was at the time; you entered &lt;code&gt;2&lt;/code&gt; when prompted and you got &lt;em&gt;species&lt;/em&gt; scaling, &lt;code&gt;-1&lt;/code&gt; got you &lt;em&gt;site&lt;/em&gt; or &lt;em&gt;sample&lt;/em&gt; scaling &lt;strong&gt;and&lt;/strong&gt; Hill’s scaling or correlation-based scores depending on whether your ordination was a linear or unimodal method. This system persisted; even in the Windows era of Canoco these numeric codes can be found lurking in the &lt;code&gt;.con&lt;/code&gt; files that describe the analysis performed. This use of numeric codes for scaling types was so pervasive that it was logical for Jari Oksanen to include the same system when the first &lt;code&gt;cca()&lt;/code&gt; and &lt;code&gt;rda()&lt;/code&gt; functions were written and in doing so Jari perpetuated one of the most frustrating things I’ve ever had to deal with as a user and teacher of ordination methods. But, as of last week, my frustration is no more…
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>My aversion to pipes</title>
        <link href="https://www.fromthebottomoftheheap.net/2015/06/03/my-aversion-to-pipes/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2015-06-03T00:00:00-06:00</published>
        <updated>2015-06-03T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2015/06/03/my-aversion-to-pipes/</id>
        
        <summary type="html">
            &lt;p&gt;
At the risk of coming across as even more of a curmudgeonly old fart than people already think I am, I really do dislike the current vogue in R that is the pipe family of binary operators; e.g. &lt;code&gt;%&amp;gt;%&lt;/code&gt;. Introduced by Hadley Wickham and popularised and advanced via the &lt;a href=&quot;http://cran.r-project.org/web/packages/magrittr/index.html&quot;&gt;&lt;strong&gt;magrittr&lt;/strong&gt; package&lt;/a&gt; by Stefan Milton Bache, the basic idea brings the forward pipe of the F# language to R. At first, I was intrigued by the prospect and initial examples suggested this might be something I would find useful. But as time has progressed and I’ve seen the use of these pipes spread, I’ve grown to dislike the idea altogether. here I outline why.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Something is rotten in the state of Denmark</title>
        <link href="https://www.fromthebottomoftheheap.net/2015/06/02/something-rotten/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2015-06-02T00:00:00-06:00</published>
        <updated>2015-06-02T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2015/06/02/something-rotten/</id>
        
        <summary type="html">
            &lt;p&gt;
On Twitter and elsewhere there has been much wailing and gnashing of teeth for some time over one particular aspect of the R ecosphere: &lt;a href=&quot;http://cran.r-project.org/&quot;&gt;CRAN&lt;/a&gt;. I’m not here to argue that everything is peachy — far from it in fact — but I am going to argue that the problems we face &lt;em&gt;do not&lt;/em&gt; begin and end with CRAN or one or more of it’s maintainers.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Drawing rarefaction curves with custom colours</title>
        <link href="https://www.fromthebottomoftheheap.net/2015/04/16/drawing-rarefaction-curves-with-custom-colours/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2015-04-16T00:00:00-06:00</published>
        <updated>2015-04-16T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2015/04/16/drawing-rarefaction-curves-with-custom-colours/</id>
        
        <summary type="html">
            &lt;p&gt;
I was sent an email this week by a &lt;strong&gt;vegan&lt;/strong&gt; user who wanted to draw rarefaction curves using &lt;code&gt;rarecurve()&lt;/code&gt; but with different colours for each curve. The solution to this one is quite easy as &lt;code&gt;rarecurve()&lt;/code&gt; has argument &lt;code&gt;col&lt;/code&gt; so the user could supply the appropriate vector of colours to use when plotting. However, they wanted to distinguish all 26 of their samples, which is certainly stretching the limits of perception if we only used colour. Instead we can vary other parameters of the plotted curves to help with identifying individual samples.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Harvesting Canadian climate data</title>
        <link href="https://www.fromthebottomoftheheap.net/2015/01/14/harvesting-canadian-climate-data/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2015-01-14T00:00:00-06:00</published>
        <updated>2015-01-14T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2015/01/14/harvesting-canadian-climate-data/</id>
        
        <summary type="html">
            &lt;p&gt;
In December I found myself helping one of our graduate students with a data problem; for one of their thesis chapters they needed a lot of hourly climate data for a handful of stations around Saksatchewan. All of this data was and is available for download from the Government of Canada’s website, but with one catch; you had to download the hourly data one month at a time, manually! There is no interface to allow a user of the website to specify the data range they want and download all the data from a single station. I figured there had to be a better way, using R to automate the downloading. Thinking the solution I came up with might help other researchers needing to grab data from the Government of Canada’s website save some time in the future, I wrote this post to document how we ended up doing it.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Analysing a randomised complete block design with vegan</title>
        <link href="https://www.fromthebottomoftheheap.net/2014/11/03/randomized-complete-block-designs-and-vegan/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2014-11-03T00:00:00-06:00</published>
        <updated>2014-11-03T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2014/11/03/randomized-complete-block-designs-and-vegan/</id>
        
        <summary type="html">
            &lt;p&gt;
It has been a long time coming. &lt;a href=&quot;http://cran.r-project.org/package=vegan&quot;&gt;&lt;strong&gt;Vegan&lt;/strong&gt;&lt;/a&gt; now has in-built, native ability to use restricted permutation designs when testing effects in constrained ordinations and in range of other methods. This new-found functionality comes courtesy of Jari (mainly) and my efforts to have vegan permutation routines use the &lt;a href=&quot;http://cran.r-project.org/package=permute&quot;&gt;&lt;strong&gt;permute&lt;/strong&gt;&lt;/a&gt; package. Jari also cooked up a standard interface that we can use to drop this and some extra features neatly into any function we want; this allows us to have permutation tests run on many CPU cores in parallel, splitting the computational burden and reducing the run time of tests, and also a mechanism that allows users to pass a matrix of user-defined permutations to be used in tests. These new features are now fully working in the development version of &lt;strong&gt;vegan&lt;/strong&gt;, which you can find on &lt;a href=&quot;https://github.com/vegandevs/vegan&quot;&gt;github&lt;/a&gt;, and which should be released to CRAN shortly. Ahead of the release, I’m preparing some examples to show off the new capabilities; first off I look at data from a randomized, complete block design experiment analysed using RDA &amp;amp; restricted permutations.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>analogue 0.14-0 released</title>
        <link href="https://www.fromthebottomoftheheap.net/2014/10/14/analogue-0.14-0-now-available-on-CRAN/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2014-10-14T00:00:00-06:00</published>
        <updated>2014-10-14T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2014/10/14/analogue-0.14-0-now-available-on-CRAN/</id>
        
        <summary type="html">
            &lt;p&gt;
A couple of week’s ago I packaged up a new release of &lt;strong&gt;analogue&lt;/strong&gt;, which is available from &lt;a href=&quot;http://cran.r-project.org/web/packages/analogue/index.html&quot;&gt;CRAN&lt;/a&gt;. Version 0.14-0 is a smaller update than the changes released in 0.12-0 and sees a continuation of the changes to dependencies to have packages in Imports rather than Depends. The main development of &lt;strong&gt;analogue&lt;/strong&gt; now takes place on &lt;a href=&quot;https://github.com/gavinsimpson/analogue/&quot;&gt;github&lt;/a&gt; and bugs and feature requests should be posted there. The Travis continuous integration system is used to automatically check the package as new code is checked in. There are several new functions and methods and a few bug fixes, the details of which are given below.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Simulating species abundance data with coenocliner</title>
        <link href="https://www.fromthebottomoftheheap.net/2014/07/31/simulating-species-abundance-data-with-the-coenocliner-package/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2014-07-31T00:00:00-06:00</published>
        <updated>2014-07-31T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2014/07/31/simulating-species-abundance-data-with-the-coenocliner-package/</id>
        
        <summary type="html">
            &lt;p&gt;
Coenoclines are, according to the Oxford Dictionary of Ecology &lt;span class=&quot;citation&quot; data-cites=&quot;Allaby1998&quot;&gt;(Allaby, 1998)&lt;/span&gt;, &lt;em&gt;“gradients of communities (e.g. in a transect from the summit to the base of a hill), reflecting the changing importance, frequency, or other appropriate measure of different species populations”&lt;/em&gt;. In much ecological research, and that of related fields, data on these coenoclines are collected and analyzed in a variety of ways. When developing new statistical methods or when trying to understand the behaviour of existing methods, we often resort to simulating data with known pattern or structure and then torture whatever method is of interest with the simulated data to tease out how well methods work or where they breakdown. There’s a long history of using computers to simulate species abundance data along coenoclines but until recently no &lt;strong&gt;R&lt;/strong&gt; packages were available that performed coenocline simulation. &lt;strong&gt;coenocliner&lt;/strong&gt; was designed to fill this gap, and today, the package was &lt;a href=&quot;http://cran.r-project.org/web/packages/coenocliner/index.html&quot;&gt;released to CRAN&lt;/a&gt;.
&lt;/p&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-Allaby1998&quot;&gt;
&lt;p&gt;
Allaby, M. (1998). &lt;em&gt;A dictionary of ecology&lt;/em&gt;. second. Oxford University Press.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Confidence intervals for derivatives of splines in GAMs</title>
        <link href="https://www.fromthebottomoftheheap.net/2014/06/16/simultaneous-confidence-intervals-for-derivatives/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2014-06-16T00:00:00-06:00</published>
        <updated>2014-06-16T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2014/06/16/simultaneous-confidence-intervals-for-derivatives/</id>
        
        <summary type="html">
            &lt;p&gt;
&lt;a href=&quot;/2011/06/12/additive-modelling-and-the-hadcrut3v-global-mean-temperature-series/&quot;&gt;Last time out&lt;/a&gt; I looked at one of the complications of time series modelling with smoothers; you have a non-linear trend which may be statistically significant but it may not be increasing or decreasing everywhere. How do we identify where in the series the data are changing? In that post I explained how we can use the first derivatives of the model splines for this purpose, and used the method of finite differences to estimate them. To assess statistical significance of the derivative (the rate of change) I relied upon asymptotic normality and the usual pointwise confidence interval. That interval is fine if looking at just one point on the spline (not of much practical use), but when considering more points at once we have a multiple comparisons issue. Instead, a simultaneous interval is required, and for that we need to revisit a technique I &lt;a href=&quot;/2011/06/12/additive-modelling-and-the-hadcrut3v-global-mean-temperature-series/&quot;&gt;blogged about a few years ago&lt;/a&gt;; posterior simulation from the fitted GAM.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Identifying periods of change in time series with GAMs</title>
        <link href="https://www.fromthebottomoftheheap.net/2014/05/15/identifying-periods-of-change-with-gams/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2014-05-15T00:00:00-06:00</published>
        <updated>2014-05-15T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2014/05/15/identifying-periods-of-change-with-gams/</id>
        
        <summary type="html">
            &lt;p&gt;
In previous posts (&lt;a href=&quot;/2011/06/12/additive-modelling-and-the-hadcrut3v-global-mean-temperature-series/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;/2011/07/21/smoothing-temporally-correlated-data/&quot;&gt;here&lt;/a&gt;) I looked at how generalized additive models (GAMs) can be used to model non-linear trends in time series data. In my previous &lt;a href=&quot;/2014/05/09/modelling-seasonal-data-with-gam/&quot;&gt;post&lt;/a&gt; I extended the modelling approach to deal with seasonal data where we model both the within year (seasonal) and between year (trend) variation with separate smooth functions. One of the complications of time series modelling with smoothers is how to summarize the fitted model; you have a non-linear trend which may be statistically significant but it may not be increasing or decreasing everywhere. How do we identify where in the series that the data are changing? That’s the topic of this post, in which I’ll use the method of finite differences to estimate the rate of change (slope) in the fitted smoother and, through some &lt;strong&gt;mgcv&lt;/strong&gt; magic, use the information recorded in the fitted model to identify periods of statistically significant change in the time series.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Modelling seasonal data with GAMs</title>
        <link href="https://www.fromthebottomoftheheap.net/2014/05/09/modelling-seasonal-data-with-gam/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2014-05-09T00:00:00-06:00</published>
        <updated>2014-05-09T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2014/05/09/modelling-seasonal-data-with-gam/</id>
        
        <summary type="html">
            &lt;p&gt;
In previous posts (&lt;a href=&quot;/2011/06/12/additive-modelling-and-the-hadcrut3v-global-mean-temperature-series/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;/2011/07/21/smoothing-temporally-correlated-data/&quot;&gt;here&lt;/a&gt;) I have looked at how generalized additive models (GAMs) can be used to model non-linear trends in time series data. At the time a number of readers commented that they were interested in modelling data that had more than just a trend component; how do you model data collected throughout the year over many years with a GAM? In this post I will show one way that I have found particularly useful in my research.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Summarising multivariate palaeoenvironmental data</title>
        <link href="https://www.fromthebottomoftheheap.net/2014/01/09/pcurve-part-2/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2014-01-09T00:00:00-06:00</published>
        <updated>2014-01-09T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2014/01/09/pcurve-part-2/</id>
        
        <summary type="html">
            &lt;p&gt;
The &lt;em&gt;horseshoe effect&lt;/em&gt; is a well known and discussed issue with principal component analysis (PCA) &lt;span class=&quot;citation&quot; data-cites=&quot;goodall_objective_1954 noy-meir_principal_1970 swan_examination_1970&quot;&gt;(e.g. Goodall, 1954; Noy-Meir and Austin, 1970; Swan, 1970)&lt;/span&gt;. Similar geometric artefacts also affect correspondence analysis (CA). In &lt;a href=&quot;/2013/12/28/pcurve-1/&quot;&gt;part 1 of this series&lt;/a&gt; I looked at the implications of these “artefacts” for the recovery of temporal or single dominant gradients from multivariate palaeoecological data. In part 2, I introduce the topic of principal curves &lt;span class=&quot;citation&quot; data-cites=&quot;hastie_principal_1989&quot;&gt;(Hastie and Stuetzle, 1989)&lt;/span&gt;.
&lt;/p&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-goodall_objective_1954&quot;&gt;
&lt;p&gt;
Goodall, D. (1954). Objective methods for the classification of vegetation. III. an essay in the use of factor analysis. &lt;em&gt;Australian Journal of Botany&lt;/em&gt; 2, 304–324. Available at: &lt;a href=&quot;http://www.publish.csiro.au/paper/BT9540304&quot;&gt;http://www.publish.csiro.au/paper/BT9540304&lt;/a&gt; [Accessed June 3, 2013].
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-hastie_principal_1989&quot;&gt;
&lt;p&gt;
Hastie, T., and Stuetzle, W. (1989). Principal curves. &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 84, 502–516. doi:&lt;a href=&quot;https://doi.org/10.2307/2289936&quot;&gt;10.2307/2289936&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-noy-meir_principal_1970&quot;&gt;
&lt;p&gt;
Noy-Meir, I., and Austin, M. P. (1970). Principal component ordination and simulated vegetational data. &lt;em&gt;Ecology&lt;/em&gt; 51, 551–552. doi:&lt;a href=&quot;https://doi.org/10.2307/1935398&quot;&gt;10.2307/1935398&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-swan_examination_1970&quot;&gt;
&lt;p&gt;
Swan, J. M. A. (1970). An examination of some ordination problems by use of simulated vegetational data. &lt;em&gt;Ecology&lt;/em&gt; 51, 89–102. doi:&lt;a href=&quot;https://doi.org/10.2307/1933602&quot;&gt;10.2307/1933602&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Decluttering ordination plots part 4: orditkplot()</title>
        <link href="https://www.fromthebottomoftheheap.net/2013/12/31/decluttering-ordination-in-vegan-part-4-orditkplot/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2013-12-31T00:00:00-06:00</published>
        <updated>2013-12-31T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2013/12/31/decluttering-ordination-in-vegan-part-4-orditkplot/</id>
        
        <summary type="html">
            &lt;p&gt;
Earlier in this series I &lt;a href=&quot;http://www.fromthebottomoftheheap.net/2013/01/12/decluttering-ordination-plots-in-vegan-part-1-ordilabel/&quot;&gt;looked at&lt;/a&gt; the &lt;code&gt;ordilabel()&lt;/code&gt; and &lt;a href=&quot;http://www.fromthebottomoftheheap.net/2013/01/13/decluttering-ordination-plots-in-vegan-part-2-orditorp/&quot;&gt;then the&lt;/a&gt; &lt;code&gt;orditorp()&lt;/code&gt; functions, and &lt;a href=&quot;http://www.fromthebottomoftheheap.net/2013/06/27/decluttering-ordination-plots-in-vegan-part-3-ordipointlabel/&quot;&gt;most recently&lt;/a&gt; the &lt;code&gt;ordipointlabel()&lt;/code&gt; function in the &lt;strong&gt;&lt;a href=&quot;http://cran.r-project.org/package=vegan&quot;&gt;vegan&lt;/a&gt;&lt;/strong&gt; package as means to improve labelling in ordination plots. In this, the fourth and final post in the series I take a look at &lt;code&gt;orditkplot()&lt;/code&gt;. If you’ve created ordination diagrams before or been following the previous posts in the irregular series, you’ll have an appreciation for the problems of drawing plots that look, well, good! Without hand editing the diagrams, there is little that even &lt;code&gt;ordipointlable()&lt;/code&gt; can do for you if you want a plot created automagically. &lt;code&gt;orditkplot()&lt;/code&gt; sits between the automated methods for decluttering ordination plots I’ve looked at previously and hand-editing in dedicated drawing software like &lt;a href=&quot;http://www.inkscape.org&quot;&gt;Inkscape&lt;/a&gt; or Illustrator, and allows some level of tweaking the locations of labelled points within R.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Summarising multivariate palaeoenvironmental data</title>
        <link href="https://www.fromthebottomoftheheap.net/2013/12/28/pcurve-1/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2013-12-28T00:00:00-06:00</published>
        <updated>2013-12-28T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2013/12/28/pcurve-1/</id>
        
        <summary type="html">
            &lt;p&gt;
Ordination methods that yield orthogonal axes of variation are often used to summarise the multivariate data obtained from sediment cores. Usually the first or, less often, the first few ordination axes are taken as directions of change or the main patterns of variance in the multivariate data. There is an oft-overlooked issue with this approach that has the potential to complicate the interpretation of the extracted axes, especially where there is a single or strong gradient in the data.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>New version of permute on CRAN</title>
        <link href="https://www.fromthebottomoftheheap.net/2013/12/17/permute-0.8-0-released/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2013-12-17T00:00:00-06:00</published>
        <updated>2013-12-17T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2013/12/17/permute-0.8-0-released/</id>
        
        <summary type="html">
            &lt;p&gt;
After some time brewing on my machines, I’m happy to have released a new version of my &lt;strong&gt;permute&lt;/strong&gt; package for R. This release took quite a while to polish and get right as there was a lot of back-and-forth between &lt;strong&gt;vegan&lt;/strong&gt; and &lt;strong&gt;permute&lt;/strong&gt; as I tried to get the latter working nicely for both useRs and developers, whilst Jari worked on using the new &lt;strong&gt;permute&lt;/strong&gt; API within &lt;strong&gt;vegan&lt;/strong&gt; itself. All these changes were prompted by Cajo ter Braak taking me to task (nicely of course) over the use in previous versions of &lt;strong&gt;permute&lt;/strong&gt; of the term “&lt;em&gt;blocks&lt;/em&gt;” for what were not true blocking factors. Cajo challenged me to add true blocking factors (these restrict permutations within their levels and are never permuted, unlike &lt;em&gt;plots&lt;/em&gt;), and the new version of &lt;strong&gt;permute&lt;/strong&gt; is the result of my attempting to meet that challenge.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>New version of analogue on CRAN</title>
        <link href="https://www.fromthebottomoftheheap.net/2013/12/14/analogue-0.12-0-released/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2013-12-14T00:00:00-06:00</published>
        <updated>2013-12-14T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2013/12/14/analogue-0.12-0-released/</id>
        
        <summary type="html">
            &lt;p&gt;
It has been almost a year since the last release of the &lt;strong&gt;analogue&lt;/strong&gt; package. At lot has happened in the intervening period and although I’ve been busy with a new job in a new country and coding on several other R packages, activity on analogue has also progressed a pace. As the version 0.12-0 of the package hits a CRAN mirror near you, I thought I’d outline the major changes in the packages, which range from &lt;em&gt;at long last&lt;/em&gt; having dissimilarity matrices computed in fast C code to lots of new functionality that makes fitting principal curves and plotting and interpreting the results much easier, from a more robust way to determine the posterior probability that two samples are analogues to rounding out the fitting of calibration models using principal components regression with ecologically-meaningful transformations.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Time series plots in R</title>
        <link href="https://www.fromthebottomoftheheap.net/2013/10/23/time-series-plots-with-lattice-and-ggplot/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2013-10-23T00:00:00-06:00</published>
        <updated>2013-10-23T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2013/10/23/time-series-plots-with-lattice-and-ggplot/</id>
        
        <summary type="html">
            &lt;p&gt;
I recently coauthored a couple of papers on trends in environmental data &lt;span class=&quot;citation&quot; data-cites=&quot;monteith-ecol-ind-trends curtis-ecol-ind-trends&quot;&gt;(Curtis and Simpson; Monteith et al.)&lt;/span&gt;, which we estimated using &lt;acronym title=&quot;Generalised Additive Models&quot;&gt;GAMs&lt;/acronym&gt;. Both papers included plots like the one shown below wherein we show the estimated trend and associated point-wise 95% confidence interval, plus some other markings. The coloured sections show where the estimated trend is changing in a statistically significantly manner, i.e. where a 95% confidence interval on the first derivative (rate of change) of the trend does not include 0. That particular figure and the others in the papers were drawn using the &lt;strong&gt;lattice&lt;/strong&gt; package &lt;span class=&quot;citation&quot; data-cites=&quot;lattice-book&quot;&gt;(Sarkar, 2008)&lt;/span&gt;, but I could just have easily used &lt;strong&gt;ggplot2&lt;/strong&gt; &lt;span class=&quot;citation&quot; data-cites=&quot;ggplot2-book&quot;&gt;(Wickham, 2009)&lt;/span&gt; instead. I was recently asked via email how I produced the figures in the paper. Rather than just reply to that email, I thought I’d knock up a quick post for my blog to show how it was done.
&lt;/p&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-curtis-ecol-ind-trends&quot;&gt;
&lt;p&gt;
Curtis, C. J., and Simpson, G. L. Trends in bulk deposition of acidity in the uk, 1988–2007, assessed using additive models. &lt;em&gt;Ecological Indicators&lt;/em&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-monteith-ecol-ind-trends&quot;&gt;
&lt;p&gt;
Monteith, D., Evans, C., Henrys, P., Simpson, G., and Malcolm, I. Trends in the hydrochemistry of acid-sensitive surface waters in the uk 1988–2008. &lt;em&gt;Ecological Indicators&lt;/em&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-lattice-book&quot;&gt;
&lt;p&gt;
Sarkar, D. (2008). &lt;em&gt;Lattice: Multivariate data visualization with r&lt;/em&gt;. New York: Springer Available at: &lt;a href=&quot;http://lmdvr.r-forge.r-project.org&quot;&gt;http://lmdvr.r-forge.r-project.org&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-ggplot2-book&quot;&gt;
&lt;p&gt;
Wickham, H. (2009). &lt;em&gt;Ggplot2: Elegant graphics for data analysis&lt;/em&gt;. Springer New York Available at: &lt;a href=&quot;http://had.co.nz/ggplot2/book&quot;&gt;http://had.co.nz/ggplot2/book&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Using Arial in R figures destined for PLOS ONE</title>
        <link href="https://www.fromthebottomoftheheap.net/2013/09/09/preparing-figures-for-plos-one-with-r/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2013-09-09T00:00:00-06:00</published>
        <updated>2013-09-09T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2013/09/09/preparing-figures-for-plos-one-with-r/</id>
        
        <summary type="html">
            &lt;p&gt;
Despite the refreshing change that the journal &lt;a href=&quot;http://www.plosone.org/&quot;&gt;PLOS ONE&lt;/a&gt; represents in terms of open access and an refreshing change to the stupidity that is quality/novelty selection by the two or three people that review a paper, it’s submission requirements are far less progressive. Yes they make you &lt;a href=&quot;http://www.plosone.org/static/figureGuidelines&quot;&gt;jump through a lot of hoops&lt;/a&gt; getting your figures and tables just so, and I can appreciate why they want some control over this in terms of the look and feel of the journal. A couple of things grate though:
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Decluttering ordination plots part 3: ordipointlabel()</title>
        <link href="https://www.fromthebottomoftheheap.net/2013/06/27/decluttering-ordination-plots-in-vegan-part-3-ordipointlabel/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2013-06-27T00:00:00-06:00</published>
        <updated>2013-06-27T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2013/06/27/decluttering-ordination-plots-in-vegan-part-3-ordipointlabel/</id>
        
        <summary type="html">
            &lt;p&gt;
Previously in this series I looked at &lt;a href=&quot;http://www.fromthebottomoftheheap.net/2013/01/12/decluttering-ordination-plots-in-vegan-part-1-ordilabel/&quot;&gt;first&lt;/a&gt; the &lt;code&gt;ordilabel()&lt;/code&gt; and &lt;a href=&quot;http://www.fromthebottomoftheheap.net/2013/01/13/decluttering-ordination-plots-in-vegan-part-2-orditorp/&quot;&gt;then&lt;/a&gt; &lt;code&gt;orditorp()&lt;/code&gt; functions in the &lt;strong&gt;&lt;a href=&quot;http://cran.r-project.org/package=vegan&quot;&gt;vegan&lt;/a&gt;&lt;/strong&gt; package as means to improve labelling in ordination plots. In this the third in the series I take a look at &lt;code&gt;ordipointlabel()&lt;/code&gt;.
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Decluttering ordination plots in vegan part 2: orditorp()</title>
        <link href="https://www.fromthebottomoftheheap.net/2013/01/13/decluttering-ordination-plots-in-vegan-part-2-orditorp/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2013-01-13T00:00:00-06:00</published>
        <updated>2013-01-13T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2013/01/13/decluttering-ordination-plots-in-vegan-part-2-orditorp/</id>
        
        <summary type="html">
            &lt;p&gt;
In the &lt;a href=&quot;http://www.fromthebottomoftheheap.net/2013/01/12/decluttering-ordination-plots-in-vegan-part-1-ordilabel/&quot; title=&quot;Decluttering ordination plots in vegan part 1: ordilabel()&quot;&gt;earlier post in this series&lt;/a&gt; I looked at the &lt;code&gt;ordilabel()&lt;/code&gt; function to help tidy up ordination biplots in &lt;a href=&quot;http://cran.r-project.org/package=vegan&quot;&gt;vegan&lt;/a&gt;. An alternative function vegan provides is &lt;code&gt;orditorp()&lt;/code&gt;, the last four letters abbreviating the words &lt;em&gt;&lt;strong&gt;t&lt;/strong&gt;ext &lt;strong&gt;or&lt;/strong&gt; &lt;strong&gt;p&lt;/strong&gt;oints&lt;/em&gt;. That is a pretty good description of what &lt;code&gt;orditorp()&lt;/code&gt; does; it draws sample or species labels using text where there is room and where there isn’t a plotting character is drawn instead. Essentially it boils down to being a one stop shop for calls to &lt;code&gt;text()&lt;/code&gt; or &lt;code&gt;points()&lt;/code&gt; as needed. Let’s see how it works…
&lt;/p&gt;

        </summary>
        
    </entry>
    
    <entry>
        <title>Decluttering ordination plots in vegan part 1: ordilabel()</title>
        <link href="https://www.fromthebottomoftheheap.net/2013/01/12/decluttering-ordination-plots-in-vegan-part-1-ordilabel/"/>
        <author>
            <name>Gavin L. Simpson</name>
        </author>
	
        <published>2013-01-12T00:00:00-06:00</published>
        <updated>2013-01-12T00:00:00-06:00</updated>
	
        <id>https://www.fromthebottomoftheheap.net/2013/01/12/decluttering-ordination-plots-in-vegan-part-1-ordilabel/</id>
        
        <summary type="html">
            &lt;p&gt;
In an &lt;a href=&quot;http://www.fromthebottomoftheheap.net/2012/04/11/customising-vegans-ordination-plots/&quot; title=&quot;Customising vegan’s ordination plots&quot;&gt;earlier post&lt;/a&gt; I showed how to customise ordination diagrams produced by our &lt;a href=&quot;http://cran.r-project.org/web/packages/vegan/index.html&quot;&gt;vegan&lt;/a&gt; package for &lt;a href=&quot;http://www.r-project.org&quot;&gt;R&lt;/a&gt; through use of colours and plotting symbols. In a series of short posts I want to cover some of the options available in vegan that can be used to help in producing better, clearer, less cluttered ordination diagrams. First up we have &lt;code&gt;ordilabel()&lt;/code&gt;.
&lt;/p&gt;

        </summary>
        
    </entry>
    
</feed>
