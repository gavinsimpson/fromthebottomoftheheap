<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Learning when, where, and by how much, things change</title>
    <meta charset="utf-8" />
    <meta name="author" content="Gavin Simpson" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="my.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: inverse, middle, left, my-title-slide, title-slide

# Learning when, where, and by how much, things change
### Gavin Simpson
### New York R Meetup • June 22 2020

---

class: inverse middle center subsection



# Use statistics to learn from data in presence of noise

???

One way to describe statistics is the principled process by which we learn from data in the presence of noise and uncertainty

---
class: inverse center middle subsection

# Learning from data&amp;hellip;?

???

Why would we want to learn from data?

---
class: inverse center middle subsection

# Estimate parameters for a theoretical model

???

Lotka-Voltera models of competition between species

---
class: inverse center middle subsection

# Compare theory with observation

???

What do the data tell us?

If we want to know how theory matches with observation then we might want to see what the data can tell us without imposing too many restrictions or constraints on our statistical model

---
class: inverse center middle subsection

# Progress with little or no theory

???

We may have little or no theory to work with, so we take an empirical approach which may lead to the development of new theory

---
class: inverse
background-image: url('./resources/franki-chamaki-z4H9MYmWIMA-unsplash.jpg')
background-size: cover

# 

.footnote[
&lt;a style="background-color:black;color:white;text-decoration:none;padding:4px 6px;font-family:-apple-system, BlinkMacSystemFont, &amp;quot;San Francisco&amp;quot;, &amp;quot;Helvetica Neue&amp;quot;, Helvetica, Ubuntu, Roboto, Noto, &amp;quot;Segoe UI&amp;quot;, Arial, sans-serif;font-size:12px;font-weight:bold;line-height:1.2;display:inline-block;border-radius:3px" href="https://unsplash.com/@franki?utm_medium=referral&amp;amp;utm_campaign=photographer-credit&amp;amp;utm_content=creditBadge" target="_blank" rel="noopener noreferrer" title="Download free do whatever you want high-resolution photos from Franki Chamaki"&gt;&lt;span style="display:inline-block;padding:2px 3px"&gt;&lt;svg xmlns="http://www.w3.org/2000/svg" style="height:12px;width:auto;position:relative;vertical-align:middle;top:-2px;fill:white" viewBox="0 0 32 32"&gt;&lt;title&gt;unsplash-logo&lt;/title&gt;&lt;path d="M10 9V0h12v9H10zm12 5h10v18H0V14h10v9h12v-9z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;&lt;span style="display:inline-block;padding:2px 3px"&gt;Franki Chamaki&lt;/span&gt;&lt;/a&gt;
]

???

We learn from data because it can highlight our preconceptions and biases

---

# Learning from data

![](new-york-r_files/figure-html/lm-plot-1.svg)&lt;!-- --&gt;

???

Learning from data could be a simple as fitting a linear regression model...

---
class: inverse
background-image: url('./resources/deep-learning-turned-up-to-11.jpg')
background-size: contain

# 

???

Or as complex as fitting a sophisticated multi-layered neural network trained on huge datasets or corpora

---

# Learning involves trade-offs

.row[
.col-6[
.center[![:scale 75%](resources/low-bias-low-variance.jpg)]
]

.col-6[
.center[![:scale 75%](resources/interpretable-black-box.jpg)]
]
]

???

Learning from data involves trade offs

We can have models that fit our data well &amp;mdash; low bias &amp;mdash; but which are highly variable, or

We can fit models that have lower variance, but these tend to have higher bias, i.e. fit the data less well

A linear regression model is very interpretable but unless the underlying relationship is linear it will have poor fit

Deep learning may fit data incredibly well but the model is very difficult to interpret and understand

---

# Generalized Additive Models

&lt;br /&gt;

![](resources/tradeoff-slider.png)

.references[Source: [GAMs in R by Noam Ross](https://noamross.github.io/gams-in-r-course/)]

???

GAMs are an intermediate-complexity model

* can learn from data without needing to be informed by the user
* remain interpretable because we can visualize the fitted features

---
class: inverse middle center large-subsection

# GAMs fit wiggly functions

---
class: inverse
background-image: url('resources/wiggly-things.png')
background-size: contain

???

---



# Wiggly things

.center[![](resources/spline-anim.gif)]

???

GAMs use splines to represent the non-linear relationships between covariates, here `x`, and the response variable on the `y` axis.

---
class: inverse middle center massive-subsection

# GAMs

---
class: inverse
background-image: url('./resources/rob-potter-398564.jpg')
background-size: contain

# GAMs are not magical

.footnote[
&lt;a style="background-color:black;color:white;text-decoration:none;padding:4px 6px;font-family:-apple-system, BlinkMacSystemFont, &amp;quot;San Francisco&amp;quot;, &amp;quot;Helvetica Neue&amp;quot;, Helvetica, Ubuntu, Roboto, Noto, &amp;quot;Segoe UI&amp;quot;, Arial, sans-serif;font-size:12px;font-weight:bold;line-height:1.2;display:inline-block;border-radius:3px;" href="https://unsplash.com/@robpotter?utm_medium=referral&amp;amp;utm_campaign=photographer-credit&amp;amp;utm_content=creditBadge" target="_blank" rel="noopener noreferrer" title="Download free do whatever you want high-resolution photos from Rob Potter"&gt;&lt;span style="display:inline-block;padding:2px 3px;"&gt;&lt;svg xmlns="http://www.w3.org/2000/svg" style="height:12px;width:auto;position:relative;vertical-align:middle;top:-1px;fill:white;" viewBox="0 0 32 32"&gt;&lt;title&gt;&lt;/title&gt;&lt;path d="M20.8 18.1c0 2.7-2.2 4.8-4.8 4.8s-4.8-2.1-4.8-4.8c0-2.7 2.2-4.8 4.8-4.8 2.7.1 4.8 2.2 4.8 4.8zm11.2-7.4v14.9c0 2.3-1.9 4.3-4.3 4.3h-23.4c-2.4 0-4.3-1.9-4.3-4.3v-15c0-2.3 1.9-4.3 4.3-4.3h3.7l.8-2.3c.4-1.1 1.7-2 2.9-2h8.6c1.2 0 2.5.9 2.9 2l.8 2.4h3.7c2.4 0 4.3 1.9 4.3 4.3zm-8.6 7.5c0-4.1-3.3-7.5-7.5-7.5-4.1 0-7.5 3.4-7.5 7.5s3.3 7.5 7.5 7.5c4.2-.1 7.5-3.4 7.5-7.5z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;&lt;span style="display:inline-block;padding:2px 3px;"&gt;Rob Potter&lt;/span&gt;&lt;/a&gt;
]

???

Despite Jared's protestations...

---
class: inverse middle center large-subsection

# Basis Expansions

---
class: inverse center middle huge-subsection

# Example

---

# HadCRUT4 time series

![](new-york-r_files/figure-html/hadcrut-temp-example-1.svg)&lt;!-- --&gt;

---

# Polynomials

![](new-york-r_files/figure-html/hadcrut-temp-polynomial-1.svg)&lt;!-- --&gt;

---
class: inverse middle center large-subsection

# Not that basis expansion

---

# Splines formed from basis functions

![](new-york-r_files/figure-html/basis-functions-1.svg)&lt;!-- --&gt;

???

Splines are built up from basis functions

Here I'm showing a cubic regression spline basis with 10 knots/functions

We weight each basis function to get a spline. Here all the basisi functions have the same weight so they would fit a horizontal line

---

# Weight basis functions &amp;#8680; spline



.center[![](resources/basis-fun-anim.gif)]

???

But if we choose different weights we get more wiggly spline

Each of the splines I showed you earlier are all generated from the same basis functions but using different weights

---

# How do GAMs learn from data?

![](new-york-r_files/figure-html/example-data-figure-1.svg)&lt;!-- --&gt;

???

How does this help us learn from data?

Here I'm showing a simulated data set, where the data are drawn from the orange functions, with noise. We want to learn the orange function from the data

---

# Maximise penalised log-likelihood &amp;#8680; &amp;beta;



.center[![](resources/gam-crs-animation.gif)]

???

Fitting a GAM involves finding the weights for the basis functions that produce a spline that fits the data best, subject to some constraints

---
class: inverse middle center subsection

# Avoid overfitting our sample

---
class: inverse middle center subsection

# Use a wiggliness penalty &amp;mdash; avoid fitting too wiggly models

---
class: inverse center middle huge-subsection

# Example

---

# HadCRUT4 time series

![](new-york-r_files/figure-html/hadcrut-temp-penalty-1.svg)&lt;!-- --&gt;

---
class: inverse center middle big-subsection

# OK some math

---
class: inverse center middle large-subsection

# How wiggly?

$$
\int_{\mathbb{R}} [f^{\prime\prime}]^2 dx = \boldsymbol{\beta}^{\mathsf{T}}\mathbf{S}\boldsymbol{\beta}
$$

---
class: inverse center middle large-subsection

# Penalised fit

$$
\mathcal{L}_p(\boldsymbol{\beta}) = \mathcal{L}(\boldsymbol{\beta}) - \frac{1}{2} \lambda\boldsymbol{\beta}^{\mathsf{T}}\mathbf{S}\boldsymbol{\beta}
$$

---

# Fitted GAM

![](new-york-r_files/figure-html/hadcrutemp-fitted-gam-1.svg)&lt;!-- --&gt;

---
class: inverse middle center massive-subsection

# mgcv

---

# Fitting GAMs in *mgcv*

Wrap a variable in `s( )` to get a smooth


```r
m &lt;- gam(Temperature ~ s(Year), data = gtemp, method = "REML")
```

--

Fit using REML or ML (`method = "ML"`) smoothness selection

GCV can undersmooth but it's the default!

---
class: inverse middle center subsection

# Climate change affecting lake temperatures?

---



# 

.references[
Data: Woolway *et al* (2019) *Climate Change* **155**, 81&amp;ndash;94 [doi: 10/c7z9](http://doi.org/c7z9)
]

![](new-york-r_files/figure-html/plot-blelham-1.svg)&lt;!-- --&gt;

---
class: inverse middle center subsection

# Why worry about minimum temperatures?

---

# Why worry about minimum temperatures?

Annual minimum temperature is a strong control on many in-lake processes (eg Hampton *et al* 2017)

Extreme events can have long-lasting effects on lake ecology &amp;mdash; mild winter in Europe 2006&amp;ndash;7 (eg Straile *et al* 2010)

Reduction in habitat or refugia for cold-adapted species

* Arctic charr (*Salvelinus alpinus*)
* Opossum shrimp (*Mysis salemaai*)

.references[Hampton *et al* (2017). Ecology under lake ice. *Ecology Letters* **20**, 98–111. [doi: 10/f3tpzh](http://doi.org/f3tpzh)

Straile *et al* (2010). Effects of a half a millennium winter on a deep lake &amp;mdash; a shape of things to come? *Global Change Biology* **16**, 2844–2856. [doi: 10/bx6t4d](http://doi.org/bx6t4d)]

---

# Multiple time series &amp;#8680; HGAM

![](new-york-r_files/figure-html/plot-all-lakes-1.svg)&lt;!-- --&gt;

---
class: inverse
background-image: url('./resources/one-does-not-simply.jpg')
background-size: contain

# 

---
class: inverse center middle large-subsection

# Central Limit Theorem

???

Central limit theorem shows us that the Gaussian or normal distribution is the sampling distribution for many sample statistics, including sample means, as samples sizes become large

Central limit theorem underlies much of the theory that justifies much of the statistics you learn about in your statistics courses, and supports the use of the Gaussian or normal distribution

---

# Annual minimum temperature

![](new-york-r_files/figure-html/plot-minima-1.svg)&lt;!-- --&gt;

---
class: inverse middle center large-subsection

# Block Minima

---

# Fisher&amp;ndash;Tippett&amp;ndash;Gnedenko theorem

&gt; The **maximum** of a sample of *iid* random variables after proper renormalization can only converge in distribution to one of three possible distributions; the *Gumbel* distribution, the *Fréchet* distribution, or the *Weibull* distribution.

.row[

.col-4[
.center[![:scale 90%](./resources/fisher.jpg)
.smaller[Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:%E0%A4%B0%E0%A5%8B%E0%A4%A8%E0%A4%BE%E0%A4%A1%E0%A5%8D.jpg)]]
]
.col-4[ 
.center[![:scale 84%](./resources/tippett.jpg)]
.smaller[Source: [ral.ucar.edu](https://ral.ucar.edu/projects/extremes/Extremes/history.html)]
]
.col-4[ 
.center[![:scale 93%](./resources/gnedenko.png)]
.smaller[Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Gnedenko.png)]
]

]

---
class: inverse middle center large-subsection

# Block Minima&amp;hellip;?

---
class: inverse middle center large-subsection

# Highly Technical Fix

---

# Negate the minima &amp;#8680; maxima

![](new-york-r_files/figure-html/plot-minima-negated-1.svg)&lt;!-- --&gt;

---
class: inverse middle center subsection

# plus some jiggery-pokery after model fitting

---
class: inverse middle center large-subsection

# Three Distributions&amp;hellip;?

---

# Generalised extreme value distribution

In 1978 Daniel McFadden demonstrated the common functional form for all three distributions — the GEVD

`$$G(y) = \exp \left \{ - \left [ 1 + \xi \left ( \frac{y - \mu}{\sigma} \right ) \right ]_{+}^{-1/\xi} \right \}$$`

.row[
.col-5[
Three parameters to estimate

* location `\(\mu\)`,
* scale `\(\sigma\)`, and
* shape `\(\xi\)`
]
.col-7[
Three distributions

* Gumbel distribution when `\(\xi\)` = 0,
* Fréchet distribution when `\(\xi\)` &gt; 0, &amp;
* Weibull distribution when `\(\xi\)` &lt; 0
]
]

---
class: inverse middle center subsection

# Fit HGAMLSS using GEV for response

---
class: inverse middle center big-subsection

# HGAMLSS&amp;hellip;?

---
class: inverse middle center subsection

# Model μ, σ, ξ with smooths of Year

---

# Estimated smooths

&amp;nbsp;

![](new-york-r_files/figure-html/draw-gev-model-smooths-1.svg)&lt;!-- --&gt;

---

# Model code

Provide a list of formulas


```r
m1 &lt;- gam(list(neg_min ~ s(cyear, lake, bs = 'fs'),
               ~ s(cyear, lake, bs = 'fs'),
               ~ s(cyear, lake, bs = 'fs')),
          data = minima, method = 'REML',
          family = gevlss(link = list('identity', 'identity', 'logit')),
          control = ctrl, optimizer = 'efs')
```

--

`bs = "fs"` is a factor-smooth interaction

* like a ransom slope &amp; intercept but for a spline
* one spline per `lake`
* one smooth parameter

---

# Observed vs fitted

![](new-york-r_files/figure-html/fitted-values-plot-1.svg)&lt;!-- --&gt;

---

# Estimated minimum temperature

![](new-york-r_files/figure-html/fitted-trends-plot-1.svg)&lt;!-- --&gt;

---

# Summary

* Lake **minimum** surface water temperatures have increased by on the order of 1&amp;ndash;3 degrees over the last 60 years

* Evidence that the distribution of annual minima has changed in many lakes &amp;mdash; implications for future extreme events which have long-term knock-on effects

* HGAMLSS with the GEV distribution are a good way of modelling common trends in environmental extremes

---
class: inverse center middle massive-subsection

# brms

---

# Fully Bayes

*mgcv* fits empirical Bayesian models with REML or ML smoothness selection

Improper Gaussian priors &amp;mdash; we don't penalise the linear bits of the basis

We can fit fully Bayesian models using *brms* with (almost) all the smooths from *mgcv*

--

Can't use `te( )` or `ti( )` for tensor product smooths (smooth interactions)

Can use `t2( )` though

---
class: inverse center middle large-subsection

# Microcystin

---

# Microcystin

A liver toxin produced by cyanobacteria

Frequent cause of negative human health effects, kills dogs, etc

Cyanobacteria can bloom under the right conditions &amp;mdash; HABs

Increases in HABs globally driven by nutrient pollution &amp; climate change

--

11 years of bi-weekly data from Qu'Appelle Valley in Saskatchewan, Canada

Hayes, N.M. *et al*, 2020. *Limnol. Oceanogr. Let.* **58**, 1736. https://doi.org/10.1002/lol2.10164

---
class: inverse center middle large-subsection

# Non-detects

---

# Fitting GAMs in brms

## non-censored version in *mgcv*


```r
mgcv_mod &lt;- gam(micro_censored ~ lake + te(DOY, cYear, by = lake),
                data = dfd, family = Gamma(link = "log"),
                method = "REML"
                control = ctrl)
```

## censored version in *brms*


```r
brms_mod &lt;- brm(microcystin | cens(censored) ~ lake + t2(DOY, cYear, by = lake),
              data = dfd, family = Gamma(link = "log"),
              warmup = 1000, iter = 3000, chains = 4, cores = CORES,
              seed = 8354, control = list(adapt_delta = 0.99))
```

---

# Fitted Microcystin


.center[![:scale 120%](resources/fitted-vs-observed-plot.svg)]

---

# Fitted Microcystin


.center[![:scale 58%](resources/posterior-mean-fitted-model-m4.png)]

---

# Fitted Microcystin


.center[![:scale 100%](resources/posterior-mean-fitted-model-m4.png)]

---

# Probability of exceeding thresholds


.center[![:scale 58%](resources/posterior-prob-exceedance-model-m4.png)]

---

# Probability of exceeding thresholds


.center[![:scale 100%](resources/posterior-prob-exceedance-model-m4.png)]

---

# Posterior predictive checks


.center[![:scale 62%](resources/posterior-predictive-checks.png)]

---

# Papers

.row[

.col-6[
![](resources/frontiers-paper-title.png)
.small[
Simpson (2018) *Frontiers in Ecology &amp; Evolution*

[doi: 10/gfrc4p](http://doi.org/gfrc4p)
]
]

.col-6[
![](resources/hgam-paper-title.png)
.small[
Pedersen *et al* (2019) *PeerJ*

[doi: 10/c6wz](http://doi.org/c6wz)
]
]

]

---

# Acknowledgements

### Funding

.row[

.col-6[
.center[![:scale 70%](./resources/NSERC_C.svg)]
]

.col-6[
.center[![:scale 70%](./resources/fgsr-logo.jpg)]
]

]

### Data

* Microcystin data from QULTER Peter Leavitt (U Regina)
* Iestyn Woolway and colleagues for archiving the lake surface water data

### Slides

* HTML Slide deck [bit.ly/nyr-gam](http://bit.ly/nyr-gam) &amp;copy; Simpson (2020) [![Creative Commons Licence](https://i.creativecommons.org/l/by/4.0/88x31.png)](http://creativecommons.org/licenses/by/4.0/)
* RMarkdown [Source](https://github.com/gavinsimpson/fromthebottomoftheheap/tree/master/slides/new-york-r-2020)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"slideNumberFormat": "<div class=\"progress-bar-container\">   <div class=\"progress-bar\" style=\"width: calc(%current% / %total% * 100%);\">   </div> </div>"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
